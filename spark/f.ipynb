{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=spark://ip-172-31-0-103:7077) created by <module> at /usr/local/lib/python2.7/dist-packages/IPython/utils/py3compat.py:288 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cc0baecc9c3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mcluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ip-172-31-0-107'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ip-172-31-0-100'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ip-172-31-0-105'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ip-172-31-0-106'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"trip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mssc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway)\u001b[0m\n\u001b[0;32m    259\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 261\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    262\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=spark://ip-172-31-0-103:7077) created by <module> at /usr/local/lib/python2.7/dist-packages/IPython/utils/py3compat.py:288 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "from geopy.distance import vincenty, Point\n",
    "\n",
    "cluster = ['ip-172-31-0-107', 'ip-172-31-0-100', 'ip-172-31-0-105', 'ip-172-31-0-106']\n",
    "\n",
    "sc = SparkContext(appName=\"trip\")\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc = StreamingContext(sc, 3)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "es = Elasticsearch(cluster, port=9200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = sc.textFile(\"test.txt\")\n",
    "D = driver.map(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assign(x):\n",
    "    ctime = x['ctime']\n",
    "    location = x['location']\n",
    "    driver = x['id']\n",
    "    name = x['name']\n",
    "    p1 = x['p1']\n",
    "    p2 = x['p2']\n",
    "    try:\n",
    "        tmp = datetime.strptime(\"{}\".format(ctime), '%Y-%m-%d %H:%M:%S.%f')\n",
    "        ctime = tmp.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    def nearby(ctime, location, driver, name, p1=None, p2=None):\n",
    "        cluster = 'ec2-52-27-127-152.us-west-2.compute.amazonaws.com'\n",
    "        es = Elasticsearch(cluster, port=9200)\n",
    "\n",
    "        geo_query = { \"from\" : 0, \"size\" : 1,\n",
    "              \"query\": {\n",
    "              \"filtered\": {\n",
    "                \"query\" : {\n",
    "                 \"term\" : {\"status\": \"wait\"}},\n",
    "                \"filter\": {\n",
    "                \"geo_distance\": {\n",
    "                    \"distance\": '3km',\n",
    "                    \"distance_type\": \"plane\", \n",
    "                    \"location\": location }}\n",
    "              }}}\n",
    "        \n",
    "        res = es.search(index='passenger', doc_type='rolling', body=geo_query )\n",
    "\n",
    "        if len(res['hits']['hits'])>0: \n",
    "            passenger = res['hits']['hits'][0][\"_source\"]\n",
    "            doc = json.dumps({\"status\": \"pickup\", \"driver\": driver})\n",
    "            q = '{{\"doc\": {}}}'.format(doc)\n",
    "            res = es.update(index='passenger', doc_type='rolling', id=passenger['id'], body=q, ignore=[400,404])\n",
    "            doc = {\"status\": \"pickup\", \"driver\": driver, \"ctime\": ctime, \"location\": location, \\\n",
    "                   'name': name, 'destination': passenger['location'], 'destinationid': passenger['id']}\n",
    "            if not p1:\n",
    "                doc['p1'] = passenger['id']\n",
    "            elif not p2:\n",
    "                doc['p2'] = passenger['id']\n",
    "            else:\n",
    "                pass # It's full!\n",
    "            doc = json.dumps(doc)\n",
    "            q = '{{\"doc\": {}}}, \"doc_as_upsert\":True'.format(doc)\n",
    "            res = es.update(index='driver', doc_type='rolling', id=passenger['id'], \\\n",
    "                            body=q, ignore=[400,404])\n",
    "            return res\n",
    "        else:\n",
    "            res = 'none'\n",
    "        return res\n",
    "    res = nearby(ctime, location, driver, name, p1, p2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idle = D.filter(lambda x: x['status']=='idle')\n",
    "if idle.count() > 1: idle.map(assign).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pickup(x):\n",
    "    if not x: return \"Hey!\"\n",
    "    ctime = x['ctime']\n",
    "    location = x['location']\n",
    "    driver = x['id']\n",
    "    name = x['name']\n",
    "    p1 = x['p1']\n",
    "    p2 = x['p2']\n",
    "    try:\n",
    "        tmp = datetime.strptime(\"{}\".format(ctime), '%Y-%m-%d %H:%M:%S.%f')\n",
    "        ctime = tmp.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    def hopOn(ctime, location, driver, name, p1=None, p2=None):\n",
    "        passenger = p2 if p2 else p1\n",
    "        p = es.get(index='passenger', doc_type='rolling', id=passenger, ignore=[404, 400])['_source']\n",
    "        if (vincenty(Point(location), Point(p['location'])).meters < 300):\n",
    "            doc = {\"status\": \"ontrip\", \"ctime\": ctime, \"location\": location,\\\n",
    "                   'destination': passenger['destination'], 'destinationid': passenger['destinationid']}\n",
    "            doc = json.dumps(doc)\n",
    "            q = '{{\"doc\": {}}}'.format(doc)\n",
    "            res = es.update(index='driver', doc_type='rolling', id=driver, \\\n",
    "                        body=q, ignore=[400,404])\n",
    "\n",
    "            ## For the sake of demo only, so that the dots are not overlaps on the map\n",
    "            newLoc = [round(location[0] - 0.0001,4), round(location[1] - 0.0001,4)]\n",
    "            if p2: newLoc_ = [round(location[0] + 0.0001,4), round(location[1] + 0.0001,4)]\n",
    "\n",
    "            doc = json.dumps({\"status\": \"ontrip\", \"ctime\": ctime, \"location\": newLoc})\n",
    "            if p2:\n",
    "                doc_ = json.dumps({\"status\": \"ontrip\", \"ctime\": ctime, \"location\": newLoc_, match: p1})\n",
    "                doc['match'] = p2\n",
    "                q_ = '{{\"doc\": {}}}'.format(doc_)\n",
    "                res = es.update(index='passenger', doc_type='rolling', id=p2, body=q_, ignore=[400,404])\n",
    "\n",
    "            q = '{{\"doc\": {}}}'.format(doc)\n",
    "            res = es.update(index='passenger', doc_type='rolling', id=p1, body=q, ignore=[400,404])\n",
    "        else:\n",
    "            res = False\n",
    "        return res\n",
    "    result = hopOn(ctime, location, driver, name, p1, p2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pickup = D.filter(lambda x: x['status']=='pickup')\n",
    "if pickup.count() > 1: idle.map(pickup).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "cluster = ['ip-172-31-0-107', 'ip-172-31-0-100', 'ip-172-31-0-105', 'ip-172-31-0-106']\n",
    "es = Elasticsearch(cluster, port=9200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_query = { \"from\" : 0, \"size\" : 1,\n",
    "          \"query\": {\n",
    "          \"filtered\": {\n",
    "            \"query\" : {\n",
    "             \"term\" : {\"status\": \"wait\"}},\n",
    "            \"filter\": {\n",
    "            \"geo_distance\": {\n",
    "                \"distance\": '3km',\n",
    "                \"distance_type\": \"plane\", \n",
    "                \"location\": [40.8083, -73.924] }}\n",
    "          }}}\n",
    "\n",
    "res = es.search(index='passenger', doc_type='rolling', body=geo_query )\n",
    "passenger = res['hits']['hits'][0]['_source']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = es.get(index='driver', doc_type='rolling', id=1, ignore=[404, 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [40.717, -73.9695]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-73.9696"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(x[0] - 0.0001,4)\n",
    "round(x[1] - 0.0001,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
